---
layout: about
title: about
permalink: /
# description: <a href="">PhD Student</a>
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: left
  image: avatar.jpg

news: true  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I'm a fourth-year PhD student at the [Center for Language and Speech Processing](https://www.clsp.jhu.edu) at Johns Hopkins University, advised by [Benjamin Van Durme](https://www.cs.jhu.edu/~vandurme/) and [Dawn Lawrie](https://hltcoe.jhu.edu/researcher/dawn-lawrie/). I am broadly interested in natural language processing (NLP), information retrieval (IR), and machine learning (ML). My research is graciously supported by a [NSF Graduate Research Fellowship](https://www.nsfgrfp.org/).

My current research interests are situated between the NLP and IR fields, where I work to improve how models find, understand, and generate information. These days my research interests fall in three main categories, although I can get distracted by other LLM-based topics:

- Retrieval models: figuring out how to [evaluate](https://arxiv.org/abs/2403.15246) [them](https://arxiv.org/abs/2305.07614) [comprehensively](https://arxiv.org/abs/2406.17186) and giving them new capabilites, such as creating [instructable/prompted retrievers](https://arxiv.org/abs/2409.11136)
- Retrieval-Augmented Generation (RAG): working towards better [RAG evaluations](https://arxiv.org/abs/2405.00982) and [improving RAG performance (often through better retrieval)](https://arxiv.org/abs/2212.10002)
- Language model pre-training data: [understanding why LMs act they way they do](https://arxiv.org/abs/2403.12958), [curating corpora for pre-training](https://arxiv.org/abs/2307.07049) or using [pre-training information to help LM generation](https://arxiv.org/abs/2305.13252)

Previously I graduated with my Bachelor's degree from Brigham Young University in computer science and statistics, where I was advised by [Kevin Seppi](https://cs.byu.edu/faculty/faculty-directory/kevin-seppi/) and [Quinn Snell](https://cs.byu.edu/faculty/faculty-directory/quinn-snell/).

In the past, I've spent time interning with many excellent mentors: at [Samaya AI](https://samaya.ai/) in 2024 with [Jack Hessel](https://jmhessel.com/), [Ashwin Paranjape](https://ashwinparanjape.github.io/), and [Yuhao Zhang](https://yuhao.im/), at Semantic Scholar/AI2 working with [Luca Soldaini](https://soldaini.net/), [Kyle Lo](https://kyleclo.github.io/), and [Arman Cohan](https://armancohan.com/) in 2023, at Apple AI/ML with [Matthias Sperber](http://msperber.com/) in 2020 and 2021, and at AllenNLP/AI2 with [Matt Gardner](https://matt-gardner.github.io/) and [Matthew Peters](https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en) in 2020.

If you're interested in getting in contact with me, please email me at {last_name}{first_name}@gmail.com.
